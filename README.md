A partir de nuestro codigo contenido en crawler.py obtenemos los datos que se usan en nuestro app.py para poder comprimirlos
y usarlos dentro de nuestra app_completa2.py encargada del funcionamieto principal de nuestro buscador, app_completa2.py hace 
la descompresion de nuestro archivo urls_new.ziphuff el cual tiene nuestros datos compresos.

La descompresion se hace mediante los metodos del objeto HuffmanCompressor definido en huffman.py

Dentro de nuestro requierements.txt podemos ver cuales son las condiciones que debe tener nuestro entorno.
